\chapter{Preliminary Development}
\label{chap-three}

There are different approaches to summarizing micro blogs. Broadly we can divide the approaches into supervised and unsupervised learning methods. The different approaches we explored are explained in this chapter. We describe the data collection and processing phases. Then we discuss the supervised classification technique. We try to classify tweets into representative and non representative tweets using generic statistical features. We discuss the results of these explorations in this chapter. We also look at unsupervised techniques like clustering using different algorithms and features.

\section{Data Collection}
Twitter provides a easy to use REST API for accessing user’s tweets. It also provides REST API client bindings in different programming languages. This makes collecting data from twitter easier compared to other micro blog services like Facebook etc. We collected data to conduct a pilot study in order to understand more about what people think are the representative tweets among a given set of tweets. We built a website where users could enter twitter username of a person they follow. They will be shown the last 30 to 50 tweets of that person. Then the users can go through these tweets and choose what they think are the representative tweets. The website is a Ruby on Rails application which used Redis key value store as the data store. The data collection application could make calls to twitter API and get the latest tweets by a particular twitter user. We did this study on a total of 13 users. This initial study helps us understand how users choose summaries from a given set of tweets. When we put together the numbers in all 541 tweets were seen amongst the 13 users. They have chosen a total of 63 tweets from these as representative tweets.

\section{Data Preparation}
The tweets collected in the data collection phase have to be pre-processed and cleaned before they can be utilized to compute features. We exclude retweets and replies by the user when we collect tweets from twitter. In this phase, the tweets are pre-processed using the two techniques Stemming and Stopword Removal. Stemming is a process where different forms of the word are reduced to its original root form. In our study, a standard porter stemmer has been used on all the words of the tweets for stemming them. Stopword Removal is a technique where commonly used english words which do not distinguish the sentence from others are removed. In this study, a stop word list in English is used to remove commonly used words in tweets like prepositions, conjunctions etc. This process ensures that all our tweets-data now have only important words eliminating  the superfluous  words that are seen in all tweets. The stop word list used in our study is provided as Appendix B of this document. The tweet data is now pre-processed and cleaned of noisy and irrelevant words and is ready to be processed. 

\section{Data Processing}
We use the dataset of 541 tweets as the dataset to conduct our exploration. We have seen that statistical features have been really good at classifying tweets \cite{Sriram:2010:STC:1835449.1835643} \cite{Sakaki:2010:EST:1772690.1772777}. Using only statistical features would make the model very generic. This will allow us to use it on different datasets. We calculated the different statistical features which can be used to classify tweets as representative tweets and non representative tweets. These statistical features include Inverse Document Frequency(IDF) for each word of each tweet.  It is a commonly used statistical metric in information retrieval. It is helpful in indicating whether a word is unique to a small set of documents or it is common across all documents. It is obtained by dividing the total number of tweets by the number of tweets containing the term, and then taking the logarithm of that quotient. Thus the IDF of a rare word is high, whereas the IDF of a frequent word is likely to be low. We also compute Spelling correctness for each word of each tweet. Then we use aggregations of these values as the features for the dataset. The set of features we chose for classifying tweets include

\begin{enumerate}
\item Number of Words in the tweet
\item Average length of words in a tweet
\item Maximum Inverse Document Frequency (IDF)
\item Position of the word with the Maximum IDF
\item Average IDF of the tweets
\item Sum of IDF of all the words in the tweet
\item Spelling correctness percentage of  a tweet
\item Number of URL’s in the tweet
\item Number of Hashtags in the tweet
\end{enumerate}

\section{Classification}
We created a comma separated values(CSV) file with all the tweets and the computed features. We did not use any features like Username or Bag of words which would make the model very specific to the dataset under consideration. We used Weka \cite{DBLP:journals/sigkdd/HallFHPRW09} to run different classification algorithms. Weka is machine learning software suite written in Java. It has a GUI and a command line interface. CSV and Arff files can be provided as inputs. There are many classification and clustering algorithms in implemented in suite. We use Weka to explore several classification and clustering algorithms.

We applied Support Vector Machine and Naive Bayes classification algorithms on our data. Support Vector Machine(SVM) is a type of supervised learning technique used for classification. SVM is a binary classifier. It classifies a given input into one of two categories. Mathematically the technique constructs a hyper plane  or set of hyper planes in a high dimensional space. This hyper plane marks the boundary between the two classes. Naive Bayes is a probabilistic classifier unlike the SVM. It is based on Bayes theorem and and makes some naive assumptions about the independence of the features, Hence the name Naive Bayes Classifier. For our exploration we try to classify our pilot study data using the above two classifier. Another classification algorithm we have explored is J48 decision trees. J48 is a Java implementation of the algorithm C4.5 to create decision trees.  This algorithm uses highest normalized information gain among available attributes at each level to build the decision tree. We have recoded the F - Measure or F1 score for each algorithm.  F - Measure is the harmonic mean of precision and recall of the classification. It acts a good measure of overall test accuracy. The F- Measures of tests using different classifiers are tabulated in table \ref{tab:one}


\begin{table}
\caption{Classification Performance}
\label{tab:one}
\begin{center}
\begin{tabular}{lccl}
\toprule
Algorithm & Precision & Recall & F-Measure\\
\midrule
Support Vector Machine & 1 & 0.036 & 0.07\\
Naive  Bayes  & 0.25 & 0.055 & 0.09\\
\bottomrule
\end{tabular}
\end{center}
\end{table}


The dataset had around 10\% of the tweets classified as representative and the rest as not representative. This created an imbalance in the dataset. Many other algorithms like Bayes Net and J48 decision trees had a F-Measure of 0 for Representative tweets. We tried to balance the dataset by sampling only 10\% of the tweets from the non representative set of tweets. This too didn't change the F-Measure much. 

\section{Clustering}
We could not achieve good performance using supervised learning techniques. Unsupervised techniques are very flexible. For an unsupervised technique we can use the bag of words and derived properties as parameters, we tagged each word of each tweet with a "parts of speech" tag. We use the Stanford core NLP\cite{stanfordcorenlplink} ruby bindings for the parts of speech tagger.  We also assigned a real number indicating the sentiment value of each word. 

Our first approach was to use bag of words as the feature set for clustering. We applied Expectation Maximization(EM)\cite{Gupta:2010:DNC:1798337.1798433} and XMeans \cite{Pelleg:2000:XEK:645529.657808} algorithm on the feature set. Euclidean distance was used as the distance metric between two tweets. Expectation Maximization is a clustering algorithm which tries to model the observed data as a mixture of Gaussians. The algorithm has two main steps Expectation Step and the Maximization Step these are run iteratively till they converge. In the Expectation step the algorithm computes the probability of a tweet being in each cluster. This is done by using log likelihood of each element attribute in comparison with the attributes of the other elements of the cluster. The next step is the Maximization step. In this step the parameters of the hidden Gaussian probability distributions are calculated. This algorithm uses a convergence test based on maximum likelihood estimation. It provides the result clusters without having to input the number of clusters to be formed. XMeans algorithm is a splitting version of KMeans which doesnt not need an input about number of clusters to be formed. At each step in XMeans a cluster is split into two smaller clusters. After the split BIS scoring is used to identify if the split is better than having just the parent cluster. The splitting goes on till all the clusters obtained result in worse BIS scoring if split. This is the criteria used for convergence in XMeans. 

 We found that clustering was broad and few clusters were created using these algorithms. Unrelated tweets also were clustered together. In order to solve these problems we realized we cannot use all the words in a tweet as features. We applied a filter on the features where only nouns and verbs were chosen to be part of the word vector. This helped in reducing the unrelated tweets being clustered together. However only few clusters were generated since the number of clusters generated in both XMeans and EM depends on maximum log likelihood estimation. This resulted in generating very few clusters as the log likelihood of many cluster did not turn out to be maximum in our case. As the few clusters generated were not sufficient, we have modified our approach. Examples of summaries formed using XMeans and EM clusters are provided in the Appendix A. For example when we applied these algorithms on 50 tweets of President Barack Obama we got only 3 clusters. 
 
Here we have included examples of Summary returned using EM and XMeans. We choose the highest sentiment tweet from each cluster as the summary tweet.

XMeans produced only two clusters for Barack Obama. One of the clusters had just one tweet so it was ignored. Results using XMeans for clustering for Barack Obama.
\begin{enumerate}
\item VPOTUS: “We’ve taken more terrorists off the battlefield in the last 3 years than in the previous 8, putting al-Qaeda on a path to defeat.”
\end{enumerate}

EM produced three clusters for Barack Obama. Results using KMeans clustering for Barack Obama.
\begin{enumerate}
\item Watch live: President Obama speaks in Colorado about preventing a student loan interest rate hike. http://t.co/mm4eGrZJ \#DontDoubleMyRate
\item Mitt Romney's at odds with Mitt Romney again—this time students are the ones who’d suffer: http://t.co/YXbQE5ek h/t @TruthTeam2012
\item Congress needs to act right now to prevent interest rates on federal students loans from shooting up and shaking you down.” President Obama
\end{enumerate}

