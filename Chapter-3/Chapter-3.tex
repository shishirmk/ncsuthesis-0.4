\chapter{Preliminary Development}
\label{chap-three}

There are different approaches to summarizing micro blogs.  Broadly we can divide the approaches into supervised and unsupervised learning approaches. The different approaches we explored are explained in this chapter. We describe the data collection and processing phases. Then we discuss the supervised classification technique. We try to classify tweets into representative and non representative tweets using generic statistical features. We discuss the result of these explorations below.  Then we look at unsupervised techniques like clustering using different algorithms and different features.

\section{Data Collection}
Twitter provides a very easy to access REST API for accessing a user’s tweets. It also provides REST API client bindings in many different languages. This makes collecting data from twitter easier compared to other micro blog services like Facebook etc. We collected data to conduct a  study in order to understand more about what people think are the representative tweets among a given set of tweets from a user. We built a website where users could enter twitter username of a person they follow. They will be shown the last 30 to 50 tweets of that person. Then the users can go through these tweets and choose what they think are the representative tweets. The website was a Ruby on Rails application which used Redis key value store as the data store. The data collection application could make calls to twitter API and get the latest tweets by a particular twitter user. We did this study on a total of 13 users. This initial study helps us understand how users choose summaries from a given set of tweets. When we put together the numbers in all 541 tweets were seen amongst the 13 users. They have chosen a total of 63 tweets from these as representative tweets.

\section{Data Preparation}
The tweets collected in the data collection phase have to be pre-processed and “cleaned” before they can be utilized to compute features.  In this phase, the tweets are pre-processed using the two techniques of “Stemming” and “Stopword Removal”. Stemming is a process where different forms of the word are reduced to its original root form. In our study, a standard porter stemmer has been used on all the words of the tweets for stemming them . They are then converted to lowercase words. Stopword Removal is a technique where commonly used english words which donot distinguish the sentence from others are removed. In this study, a stop word list in English is used to remove commonly used words in tweets like prepositions, conjunction. This process ensures that all our tweets-data now have only important words eliminating  the superfluous  words that are seen in all tweets but act as noisy data in our study. The stop word list used in our study is provided as an attachment in the appendix section of this document. The tweet data is now pre-processed and cleaned of noisy and irrelevant words and is ready to be processed. 

\section{Data Processing}
We use the dataset of 541 tweets as the dataset to conduct our exploration. We have seen that statistical features have been really good at classifying tweets. Using only statistical features would make the model very generic. This will allow us to use it on different datasets. We calculated the different statistical features which can be used to classify tweets as representative tweets and non representative tweets. These statistical features include Inverse Document Frequency(IDF) for each word of each tweet. It is a commonly used statistical metric in information retrieval. It is helpful in indicating whether a word is unique to a small set of documents or it is common across all documents.We also compute Spelling correctness for each word of each tweet. Then we use aggregations of these values as the features for the dataset. 

\begin{enumerate}
\item The set of features we chose for classifying tweets include
\item Number of Words in the tweet
\item Average length of words in a tweet
\item Maximum Inverse Document Frequency (IDF)
\item Position of the word with the Maximum IDF
\item Average IDF of the tweets
\item Sum of IDF of all the words in the tweet
\item Spelling correctness percentage of  a tweet
\item Number of URL’s in the tweet
\item Number of Hashtags in the tweet
\end{enumerate}

\section{Classification}
We created a comma separated values(CSV) file with all the tweets and the computed features. We did not any features like Username or Bag of words etc which would make the model very specific to the dataset under consideration. We used Weka to run different classification algorithms and noted down the F-Measure. The results are tabulated in table \ref{tab:one}


\begin{table}
\caption{Classification Performance}
\label{tab:one}
\begin{center}
\begin{tabular}{lccl}
\toprule
Algorithm & Precision & Recall & F-Measure\\
\midrule
Support Vector Machine & 1 & 0.036 & 0.07\\
Naive  Bayes  & 0.25 & 0.055 & 0.09\\
\bottomrule
\end{tabular}
\end{center}
\end{table}


The dataset had around 10\% of the tweets classified as representative and the rest as not representative. This created an imbalance in the dataset many other algorithms like Bayes Net and J48 decision trees had a F-Measure of 0 for Representative tweets. We tried to balance the dataset by sampling only 10\% of the tweets from the non representative set of tweets. This too didn't change the F-Measure much. 

\section{Clustering}
We could not achieve good performance using supervised learning techniques. Unsupervised techniques are very flexible. For an unsupervised technique we can use the words and properties of the words in a tweet as parameters, we tagged each word of each tweet with a parts of speech tag. We also assigned a real number indicating the sentiment value of each word. 

Our first approach was to use bag of words as the feature set for clustering. We applied Expectation Maximization(EM) and XMeans algorithm on the feature set. Euclidean distance was used as the distance metric between two tweets. We found that clustering was broad and few clusters were created using these algorithms. Unrelated tweets also were clustered together. In order to solve these problems we realized we cannot use all the words in a tweet as features. We applied a filter on the features only nouns and verbs were chosen to be part of the word vector. This helped is reducing the unrelated tweets being clustered together. However only few clusters were generated. Since the number of clusters generated in both XMeans and EM depends on maximum log likelihood estimation. The log likelihood of more clusters would not be maximum but still only few clusters were not enough. Examples of summaries formed using XMeans and EM clusters are provided in the Appendix. For example when we applied these algorithms on 50 tweets of President Barack Obama we got only 3 clusters. 

